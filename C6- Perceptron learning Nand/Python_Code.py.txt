import numpy as np

# Sigmoid activation and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_deriv(x):
    return sigmoid(x) * (1 - sigmoid(x))

# Perceptron class
class Perceptron:
    def __init__(self, learning_rate=0.1):
        self.w = np.random.randn(2)
        self.b = np.random.randn()
        self.lr = learning_rate

    def forward(self, x):
        z = np.dot(self.w, x) + self.b
        return sigmoid(z)

    def train(self, X, y, epochs=10000):
        for _ in range(epochs):
            for xi, target in zip(X, y):
                output = self.forward(xi)
                error = target - output
                self.w += self.lr * error * xi
                self.b += self.lr * error

    def predict(self, x):
        return 1 if self.forward(x) >= 0.5 else 0


nand_data = np.array([[0,0], [0,1], [1,0], [1,1]])
nand_labels = np.array([1, 1, 1, 0])

p_nand = Perceptron()
p_nand.train(nand_data, nand_labels)

print("NAND Results:")
for x in nand_data:
    print(f"{x} -> {p_nand.predict(x)}")
